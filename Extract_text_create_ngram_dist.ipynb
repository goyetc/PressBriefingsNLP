{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import itertools\n",
    "import os\n",
    "import codecs\n",
    "import csv\n",
    "import glob\n",
    "import ssl\n",
    "import nltk\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def briefing_extractor_list(urllist):\n",
    "\n",
    "    for url in urllist:\n",
    "        \n",
    "        #url = \"https://www.whitehouse.gov/the-press-office/2017/01/21/statement-press-secretary-sean-spicer\"\n",
    "\n",
    "        #accommodate some sites that do not have valid https verification running.\n",
    "        #WARNING: BE AWARE OF WHAT SITE YOU ARE PULLING INFORMATION FROM\n",
    "\n",
    "        context = ssl._create_unverified_context()\n",
    "        html = urllib.urlopen(url, context=context).read()\n",
    "\n",
    "        soup = BeautifulSoup(html)\n",
    "        \n",
    "        subname = \"briefings\"\n",
    "        \n",
    "        filename = os.path.join(\"EDAVtest_\" + url[url.index(subname):]+ \".\" + \"txt\")\n",
    "        filename = filename.replace('/','_')\n",
    "\n",
    "        #print filename\n",
    "        # kill all script and style elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()    # rip it out\n",
    "\n",
    "        # get text\n",
    "        text = soup.body.get_text()\n",
    "\n",
    "        # break into lines and remove leading and trailing space on each\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        # break multi-headlines into a line each\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        # drop blank lines\n",
    "        text = '\\n'.join(chunk for chunk in chunks if chunk).encode('ascii', 'ignore')\n",
    "\n",
    "        #identify header/footer, extract text between\n",
    "        header = 'FLEISCHER:'\n",
    "         \n",
    "        footer = 'END'\n",
    "        \n",
    "        body = text[text.index(header):text.index(footer)]\n",
    "\n",
    "        #print body or write to file\n",
    "\n",
    "        #print body\n",
    "        \n",
    "        with open(filename, \"w\") as f:\n",
    "                f.write(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "urllist = (\"https://georgewbush-whitehouse.archives.gov/news/briefings/20010125.html\", \n",
    "           \"https://georgewbush-whitehouse.archives.gov/news/briefings/20010125-1.html\", \n",
    "           \"https://georgewbush-whitehouse.archives.gov/news/briefings/20010126.html\", \n",
    "           \"https://georgewbush-whitehouse.archives.gov/news/briefings/20010129.html\", \n",
    "           \"https://georgewbush-whitehouse.archives.gov/news/briefings/20010130.html\", \n",
    "           \"https://georgewbush-whitehouse.archives.gov/news/briefings/20010131.html\", \n",
    "           \"https://georgewbush-whitehouse.archives.gov/news/briefings/20010201.html\", \n",
    "           \"https://georgewbush-whitehouse.archives.gov/news/briefings/20010202.html\", \n",
    "           \"https://georgewbush-whitehouse.archives.gov/news/briefings/20010205.html\", \n",
    "           \"https://georgewbush-whitehouse.archives.gov/news/briefings/20010206.html\", \n",
    "           \"https://georgewbush-whitehouse.archives.gov/news/briefings/20010207.html\", \n",
    "           \"https://georgewbush-whitehouse.archives.gov/news/briefings/20010208.html\", \n",
    "           \"https://georgewbush-whitehouse.archives.gov/news/briefings/20010209.html\", \n",
    "           \"https://georgewbush-whitehouse.archives.gov/news/briefings/20010215.html\", \n",
    "           \"https://georgewbush-whitehouse.archives.gov/news/briefings/20010226.html\", \n",
    "           \"https://georgewbush-whitehouse.archives.gov/news/briefings/20010227.html\", \n",
    "           \"https://georgewbush-whitehouse.archives.gov/news/briefings/20010302.html\", \n",
    "           \"https://georgewbush-whitehouse.archives.gov/news/briefings/20010305.html\", \n",
    "           \"https://georgewbush-whitehouse.archives.gov/news/briefings/20010307.html\", \n",
    "           \"https://georgewbush-whitehouse.archives.gov/news/briefings/20010313.html\", \n",
    "           \"https://georgewbush-whitehouse.archives.gov/news/briefings/20010320.html\", \n",
    "           \"https://georgewbush-whitehouse.archives.gov/news/briefings/20010322.html\", \n",
    "           \"https://georgewbush-whitehouse.archives.gov/news/briefings/20010323.html\", \n",
    "           \"https://georgewbush-whitehouse.archives.gov/news/briefings/20010326.html\", \n",
    "           \"https://georgewbush-whitehouse.archives.gov/news/briefings/20010327.html\", \n",
    "           \"https://georgewbush-whitehouse.archives.gov/news/briefings/20010328.html\", \n",
    "           #\"https://georgewbush-whitehouse.archives.gov/news/briefings/20010330.html\", \n",
    "           #note.. even though labeled as Fleischer, was actually Scott McClellan\n",
    "           \"https://georgewbush-whitehouse.archives.gov/news/briefings/20010330-1.html\",\n",
    "           \"https://georgewbush-whitehouse.archives.gov/news/briefings/20010404.html\",\n",
    "           \"https://georgewbush-whitehouse.archives.gov/news/briefings/20010405.html\",\n",
    "           \"https://georgewbush-whitehouse.archives.gov/news/briefings/20010406.html\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "briefing_extractor_list(urllist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Append all individual briefings to single text file, fix encoding for subsequent NLTK steps\n",
    "\n",
    "text = '*EDAVtest*.txt'\n",
    "\n",
    "read_files = glob.glob(text)\n",
    "\n",
    "filename = 'EDAVtest_compiled.txt'\n",
    "with open(filename, \"wb\") as outfile:\n",
    "    for f in read_files:\n",
    "        with open(f, \"rb\") as infile:\n",
    "            \n",
    "            #outfile.write(infile.read())\n",
    "            in_byte_string = infile.read()\n",
    "            unicode_string = in_byte_string.decode('UTF-8')\n",
    "            out_byte_string = unicode_string.encode('ASCII', 'ignore')\n",
    "\n",
    "            outfile.write(out_byte_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def produce_frequency_distributions(text_file):\n",
    "\n",
    "        \n",
    "        #establish stop words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "\n",
    "        #some updates based on manual txt analysis\n",
    "        stop_words.update(['mr', 'fleischer','mcclellan', '.', ',', '?', '!', ';', '(', ')', '[', ']', '{', '}', '-', '--', ':', 'q', \"'\", '#', '.)'])\n",
    "\n",
    "        #remove stop words from text file\n",
    "        with open(text_file, \"r\") as f1:\n",
    "            raw = f1.read()\n",
    "            #raw_nostop = [i for i in raw.lower().split() if i not in stop_words]\n",
    "            raw_nostop = [i.lower() for i in wordpunct_tokenize(raw) if i.lower() not in stop_words]\n",
    "            str_nostop = \" \".join(raw_nostop)\n",
    "\n",
    "        #tokenize str extracted from text file\n",
    "        tokens = nltk.word_tokenize(str_nostop)\n",
    "\n",
    "        #Create n-grams\n",
    "        bgs = nltk.bigrams(tokens)\n",
    "        tgs = nltk.trigrams(tokens)\n",
    "\n",
    "        \n",
    "            \n",
    "        #compute frequency distributions: unigrams, bigrams, trigrams\n",
    "        bigrams = nltk.FreqDist(bgs)\n",
    "        bi = \"bigrams\"\n",
    "        trigrams = nltk.FreqDist(tgs)\n",
    "        tri = \"trigrams\"\n",
    "        unigrams = nltk.FreqDist(tokens)\n",
    "        uni = \"unigrams\"\n",
    "        gramslist = [(unigrams,uni), (bigrams,bi), (trigrams,tri)]\n",
    "        \n",
    "        for grams in gramslist:\n",
    "            \n",
    "            #write to csv: note- change file name and writer.writerows arg as needed. also, fuck trump.\n",
    "            with open('%s%s.csv' % (text_file[:text_file.index(\"_\")], grams[1]), \"wb\") as fuck_trump:\n",
    "                writer = csv.writer(fuck_trump, quoting=csv.QUOTE_ALL)\n",
    "                writer.writerows(grams[0].items())\n",
    "\n",
    "            #for k,v in fdist.items():\n",
    "                #print k,v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "produce_frequency_distributions(\"EDAVtest_compiled.txt\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
